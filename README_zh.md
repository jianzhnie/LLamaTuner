<div align="center">
  <img src="assets/guanaco.svg" width="300"/>
<div>&nbsp;</div>
</div>

![GitHub Repo stars](https://img.shields.io/github/stars/jianzhnie/Chinese-Guanaco?style=social)
![GitHub Code License](https://img.shields.io/github/license/jianzhnie/Chinese-Guanaco)
![GitHub last commit](https://img.shields.io/github/last-commit/jianzhnie/Chinese-Guanaco)
![GitHub pull request](https://img.shields.io/badge/PRs-welcome-blue)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

<div align="center">

ğŸ‘‹ğŸ¤—ğŸ¤—ğŸ‘‹ åŠ å…¥æˆ‘ä»¬ [WeChat](assets/wechat.jpg).
</div>


# Efficient Finetuning of Quantized LLMs --- ä½èµ„æºçš„å¤§è¯­è¨€æ¨¡å‹é‡åŒ–è®­ç»ƒ/éƒ¨ç½²æ–¹æ¡ˆ


<div align="center">

[English](README.md) | ä¸­æ–‡
</div>

è¿™é‡Œæ˜¯`Efficient Finetuning of Quantized LLMs`é¡¹ç›®çš„å­˜å‚¨åº“ï¼Œæ—¨åœ¨æ„å»ºå’Œå¼€æº éµå¾ªæŒ‡ä»¤çš„`baichuan/LLaMA/Pythia/GLM`ä¸­æ–‡å¤§æ¨¡å‹å¾®è°ƒè®­ç»ƒæ–¹æ³•ï¼Œè¯¥æ–¹æ³•å¯ä»¥åœ¨**å•ä¸ª Nvidia RTX-2080TI**ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¤šè½®èŠå¤©æœºå™¨äººå¯ä»¥åœ¨**å•ä¸ª Nvidia RTX-3090**ä¸Šè¿›è¡Œä¸Šä¸‹æ–‡é•¿åº¦ 2048çš„æ¨¡å‹è®­ç»ƒã€‚

æˆ‘ä»¬ä½¿ç”¨[bitsandbytes](https://github.com/TimDettmers/bitsandbytes)è¿›è¡Œé‡åŒ–ï¼Œå¹¶ä¸Huggingfaceçš„[PEFT](https://github.com/huggingface/peft)å’Œ [transformers](https://github.com/huggingface/transformers/)åº“é›†æˆã€‚

 æœ¬é¡¹ç›®ä¸»è¦å†…å®¹å¦‚ä¸‹ï¼š

- ğŸ“— æ”¯æŒå…¨é‡å‚æ•°æŒ‡ä»¤å¾®è°ƒã€LoRAæŒ‡ä»¤å¾®è°ƒ(åç»­å°†ä¼šæä¾›æ”¯æŒ)ï¼Œ QLoRAä½æˆæœ¬é«˜æ•ˆæŒ‡ä»¤å¾®è°ƒã€‚
- ğŸ“— æ”¯æŒç»å¤§éƒ¨åˆ†ä¸»æµçš„å¼€æºå¤§æ¨¡å‹ï¼Œå¦‚ç™¾å· baichuanã€Ziyaã€Bloomã€LLaMAã€Pythiaã€OPTç­‰ã€‚
- ğŸ“— æ”¯æŒloraä¸base modelè¿›è¡Œæƒé‡åˆå¹¶ï¼Œæ¨ç†æ›´ä¾¿æ·ã€‚
- ğŸ“— å¼€æºå’Œæ•´ç†æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†çš„æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†è„šæœ¬ã€‚
- ğŸ“— å¼€æº[ä¸€ç³»åˆ—æŒ‡ä»¤å¾®è°ƒæ¨¡å‹æƒé‡](https://huggingface.co/GaussianTech/) ã€‚

<details><summary><b>Qlora ç®€ä»‹:</b></summary>

QLora æ˜¯ä¸€ç§æœ‰æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¯ä»¥åœ¨å•ä¸ª48GB GPUä¸Šå¾®è°ƒ65Bå‚æ•°æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒå®Œæ•´çš„16ä½å¾®è°ƒä»»åŠ¡æ€§èƒ½ã€‚QLora ä½¿ç”¨ä¸€ç§ä½ç²¾åº¦çš„å­˜å‚¨æ•°æ®ç±»å‹ï¼ˆNF4ï¼‰æ¥å‹ç¼©é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚é€šè¿‡å†»ç»“ LM å‚æ•°ï¼Œå°†ç›¸å¯¹å°‘é‡çš„å¯è®­ç»ƒå‚æ•°ä»¥ Low-Rank Adapters çš„å½¢å¼æ·»åŠ åˆ°æ¨¡å‹ä¸­ï¼ŒLoRA å±‚æ˜¯åœ¨è®­ç»ƒæœŸé—´æ›´æ–°çš„å”¯ä¸€å‚æ•°ï¼Œä½¿å¾—æ¨¡å‹ä½“é‡å¤§å¹…å‹ç¼©åŒæ—¶æ¨ç†æ•ˆæœå‡ ä¹æ²¡æœ‰å—åˆ°å½±å“ã€‚ä»QLoraçš„åå­—å¯ä»¥çœ‹å‡ºï¼ŒQLoraå®é™…ä¸Šæ˜¯Quantize+LoRAæŠ€æœ¯ã€‚

æˆ‘ä»¬å¼€æºçš„ Guanaco æ¨¡å‹åœ¨ Vicuna åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºæ‰€æœ‰ä»¥å‰çš„å…¬å¼€å‘å¸ƒæ¨¡å‹ï¼Œè¾¾åˆ°äº† ChatGPT çš„æ€§èƒ½æ°´å¹³ 99.3%ï¼Œè€Œåœ¨å•ä¸ª GPU ä¸Šåªéœ€è¦ 24 å°æ—¶çš„å¾®è°ƒã€‚

QLora å¼•å…¥äº†å¤šç§åˆ›æ–°ï¼Œæ—¨åœ¨åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å‡å°‘å†…å­˜ä½¿ç”¨ï¼š

1. 4-bit NormalFloatï¼šè¿™æ˜¯ä¸€ç§ç†è®ºä¸Šé’ˆå¯¹æ­£æ€åˆ†å¸ƒæ•°æ®çš„æœ€ä¼˜çš„é‡åŒ–æ•°æ®ç±»å‹ï¼Œä¼˜äºå½“å‰æ™®éä½¿ç”¨çš„FP4ä¸Int4ã€‚
2. Double Quantizationï¼šç›¸æ¯”äºå½“å‰çš„æ¨¡å‹é‡åŒ–æ–¹æ³•ï¼Œæ›´åŠ èŠ‚çœæ˜¾å­˜ç©ºé—´ã€‚æ¯ä¸ªå‚æ•°å¹³å‡èŠ‚çœ0.37bitï¼Œå¯¹äº65Bçš„LLaMAæ¨¡å‹ï¼Œå¤§çº¦èƒ½èŠ‚çœ3GBæ˜¾å­˜ç©ºé—´ã€‚
3. Paged Optimizersï¼šä½¿ç”¨NVIDIAç»Ÿä¸€å†…å­˜æ¥é¿å…åœ¨å¤„ç†å°æ‰¹é‡çš„é•¿åºåˆ—æ—¶å‡ºç°çš„æ¢¯åº¦ Checkppints å†…å­˜å³°å€¼ã€‚
4. å¢åŠ  Adapterï¼š4-bit NormalFloatä¸Double Quantizationï¼ŒèŠ‚çœäº†å¾ˆå¤šç©ºé—´ï¼Œä½†å¸¦æ¥äº†æ€§èƒ½æŸå¤±ï¼Œä½œè€…é€šè¿‡æ’å…¥æ›´å¤šadapteræ¥å¼¥è¡¥è¿™ç§æ€§èƒ½æŸå¤±ã€‚åœ¨LoRAä¸­ï¼Œä¸€èˆ¬ä¼šé€‰æ‹©åœ¨queryå’Œvalueçš„å…¨è¿æ¥å±‚å¤„æ’å…¥adapterã€‚è€ŒQLoraåˆ™åœ¨æ‰€æœ‰å…¨è¿æ¥å±‚å¤„éƒ½æ’å…¥äº†adapterï¼Œå¢åŠ äº†è®­ç»ƒå‚æ•°ï¼Œå¼¥è¡¥ç²¾åº¦å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

å®Œæ•´ä»‹ç»æŸ¥çœ‹ï¼š[QLORA: Efficient Finetuning of Quantized LLMs](https://jianzhnie.github.io/machine-learning-wiki/#/ai-general/quantization/qlora)

</details>

## æ–°é—»

- [23/06/25] æˆ‘ä»¬å‘å¸ƒäº†æœ‰ç›‘ç£çš„finetune baichuan-7Bæ¨¡å‹ï¼ˆ[GaussianTech/baichuan-7b-sft](https://huggingface.co/GaussianTech/baichuan-7b-sft)ï¼‰å’Œç›¸åº”çš„è®­ç»ƒè„šæœ¬ã€‚
- [23/06/24] æˆ‘ä»¬å‘å¸ƒäº†æœ‰ç›‘ç£çš„finetune llama-7Bæ¨¡å‹ï¼ˆ[GaussianTech/llama-7b-sft](https://huggingface.co/GaussianTech/llama-7b-sft)ï¼‰å’Œç›¸åº”çš„è®­ç»ƒè„šæœ¬ã€‚
- [23/06/15] ç°åœ¨æˆ‘ä»¬åœ¨è¿™ä¸ªä»“åº“ä¸­æ”¯æŒè®­ç»ƒ baichuan-7B æ¨¡å‹ï¼Œ å°è¯•`--model_name_or_path baichuan-inc/baichuan-7B`ä½¿ç”¨baichuan-7Bå‹å·ã€‚
- [23/06/03] ç°åœ¨æˆ‘ä»¬æ”¯æŒé‡åŒ–è®­ç»ƒå’Œæ¨ç†ï¼ˆåˆå QLoRAï¼‰ï¼Œå°è¯•`scripts/qlora_finetune/finetune_llama_guanaco7b.sh`å¹¶è®¾ç½®`--bits 4/8`å‚æ•°ä»¥ä½¿ç”¨é‡åŒ–æ¨¡å‹ã€‚
- [23/05/25] ç°åœ¨æ”¯æŒLoraè®­ç»ƒå’Œæ¨ç†ï¼Œ å°è¯• `scripts/lora_finetune/lora-finetune_alpaca.sh` åœ¨ Alpaca æ•°æ®é›†ä¸Šä½¿ç”¨ Lora å¾®è°ƒ LLAMA æ¨¡å‹ã€‚
- [20/05/23] ç›®å‰æ”¯æŒå…¨å‚æ•°è°ƒä¼˜å’Œéƒ¨åˆ†å‚æ•°å¾®è°ƒï¼Œå°è¯•`scripts/full_finetune/full-finetune_alpaca.sh` åœ¨Alpaca æ•°æ®é›†ä¸Šå®Œå…¨å¾®è°ƒ LLAMA æ¨¡å‹ã€‚

## æ”¯æŒçš„æ¨¡å‹

- [LLaMA](https://github.com/facebookresearch/llama) (7B/13B/33B/65B)
- [BLOOM](https://huggingface.co/bigscience/bloom) & [BLOOMZ](https://huggingface.co/bigscience/bloomz) (560M/1.1B/1.7B/3B/7.1B/176B)
- [baichuan](https://huggingface.co/baichuan-inc/baichuan-7B) (7B)
- [OPT](https://huggingface.co/docs/transformers/model_doc/opt) (125M/350M/1.3B/2.7B/6.7B/66B )

## æ”¯æŒçš„è®­ç»ƒæ–¹æ³•

- (Continually) pre-training
  - Full-parameter tuning
  - Partial-parameter tuning
  - [LoRA](https://arxiv.org/abs/2106.09685)
  - [QLoRA](https://arxiv.org/abs/2305.14314)
- Supervised fine-tuning
  - Full-parameter tuning
  - Partial-parameter tuning
  - [LoRA](https://arxiv.org/abs/2106.09685)
  - [QLoRA](https://arxiv.org/abs/2305.14314)

## æä¾›çš„æ•°æ®é›†æ¥å£

- For supervised fine-tuning:
  - [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca)
  - [Stanford Alpaca (Chinese)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
  - [Hello-SimpleAI/HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3)
  - [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)
  - [mosaicml/dolly_hhrlhf](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf)
  - [GPT-4 Generated Data](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)
  - [Alpaca CoT](https://huggingface.co/datasets/QingyiSi/Alpaca-CoT)
  - [UltraChat](https://github.com/thunlp/UltraChat)
  - [OpenAssistant/oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)
  - [ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered)
  - [BIAI/OL-CC](https://data.baai.ac.cn/details/OL-CC)
  - [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)
  - [Evol-Instruct](https://huggingface.co/datasets/victor123/evol_instruct_70k)
- For reward model training:
  - [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf)
  - [Open Assistant](https://huggingface.co/datasets/OpenAssistant/oasst1)
  - [GPT-4 Generated Data](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)
  - [GPT-4 Generated Data (Chinese)](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)

## æ¨¡å‹ä»“åº“

æˆ‘ä»¬åœ¨ [Hugging Face ](https://huggingface.co/GaussianTech/)æä¾›äº†è®¸å¤šæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹ç»è¿‡Self- Instruct æ•°æ®é›†çš„è®­ç»ƒï¼Œå¯ç”¨äºæ¨ç†å’Œå¾®è°ƒï¼š

ğŸ”” ä½¿ç”¨æœ¬é¡¹ç›®çš„è®­ç»ƒä»£ç ï¼Œä»¥åŠä¸Šè¿°è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬è®­ç»ƒå¹¶å¼€æºäº†ä»¥ä¸‹æ¨¡å‹ã€‚

| Base Model                                                   | Adapter      | Instruct Datasets                                            | Model on Huggingface                                         |
| ------------------------------------------------------------ | ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [llama-7b](https://huggingface.co/decapoda-research/llama-7b-hf) | FullFinetune | -                                                            |                                                              |
| [llama-7b](https://huggingface.co/decapoda-research/llama-7b-hf) | QLoRA        | [openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) | [GaussianTech/llama-7b-sft](https://huggingface.co/GaussianTech/llama-7b-sft) |
| [llama-7b](https://huggingface.co/decapoda-research/llama-7b-hf) | QLoRA        | [OL-CC](https://data.baai.ac.cn/details/OL-CC)               |                                                              |
| [baichuan7b](https://huggingface.co/baichuan-inc/baichuan-7B) | QLoRA        | [openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) | [GaussianTech/baichuan-7b-sft](https://huggingface.co/GaussianTech/baichuan-7b-sft) |
| [baichuan7b](https://huggingface.co/baichuan-inc/baichuan-7B) | QLoRA        | [OL-CC](https://data.baai.ac.cn/details/OL-CC)               | -                                                            |

## å®‰è£…

### è¦æ±‚

- CUDA >= 11.0
- Python 3.8+ å’Œ PyTorch 1.13.1+
- ğŸ¤—Transformersã€æ•°æ®é›†ã€Accelerateã€PEFT å’Œ bitsandbytes
- jiebaã€rouge_chinese å’Œ nltkï¼ˆè¯„ä¼°æ—¶ä½¿ç”¨ï¼‰
- gradioï¼ˆåœ¨gradio_webserver.pyä¸­ä½¿ç”¨ï¼‰

### å®‰è£…æ‰€éœ€çš„åŒ…

è¦ä½¿ç”¨ Transformer å’Œ BitsandBytes åŠ è½½ 4 ä½æ¨¡å‹ï¼Œæ‚¨å¿…é¡»ä»æºä»£ç å®‰è£…åŠ é€Ÿå™¨å’Œ Transformerï¼Œå¹¶ç¡®ä¿æ‚¨æ‹¥æœ‰æœ€æ–°ç‰ˆæœ¬çš„ BitsandBytes åº“ (0.39.0)ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥å®ç°ä¸Šè¿°ç›®çš„ï¼š

```shell
pip install -q -U bitsandbytes
pip install -q -U git+https://github.com/huggingface/transformers.git
pip install -q -U git+https://github.com/huggingface/peft.git
pip install -q -U git+https://github.com/huggingface/accelerate.git
```

### å…‹éš†ä»£ç 

å…‹éš†æ­¤å­˜å‚¨åº“å¹¶å¯¼èˆªåˆ° Efficient-Tuning-LLMs æ–‡ä»¶å¤¹

```shell
git clone https://github.com/jianzhnie/Efficient-Tuning-LLMs.git
cd Efficient-Tuning-LLMs
```

## å¿«é€Ÿå¼€å§‹

### QLora int4 å¾®è°ƒ

è¯¥`train_qlora.py`ä»£ç æ˜¯å¯¹å„ç§æ•°æ®é›†è¿›è¡Œå¾®è°ƒå’Œæ¨ç†çš„èµ·ç‚¹ã€‚åœ¨ Alpaca æ•°æ®é›†ä¸Šå¾®è°ƒåŸºçº¿æ¨¡å‹çš„åŸºæœ¬å‘½ä»¤ï¼š

```shell
python train_qlora.py --model_name_or_path <path_or_name>
```

å¯¹äºå¤§äº13Bçš„æ¨¡å‹ï¼Œæˆ‘ä»¬å»ºè®®è°ƒæ•´å­¦ä¹ ç‡ï¼š

```shell
python train_qlora.py â€“learning_rate 0.0001 --model_name_or_path <path_or_name>
```

æˆ‘ä»¬è¿˜å¯ä»¥è°ƒæ•´æˆ‘ä»¬çš„è¶…å‚æ•°ï¼š

```python
python train_qlora.py \
    --model_name_or_path ~/checkpoints/baichuan7b \
    --dataset_name oasst1 \
    --data_dir ~/prompt_datasets \
    --load_from_local \
    --output_dir ./work_dir/oasst1-baichuan-7b \
    --num_train_epochs 4 \
    --per_device_train_batch_size 4 \
    --per_device_eval_batch_size 4 \
    --gradient_accumulation_steps 8 \
    --evaluation_strategy steps \
    --eval_steps 50 \
    --save_strategy steps \
    --save_total_limit 5 \
    --save_steps 100 \
    --logging_strategy steps \
    --logging_steps 1 \
    --learning_rate 0.0002 \
    --warmup_ratio 0.03 \
    --weight_decay 0.0 \
    --lr_scheduler_type constant \
    --adam_beta2 0.999 \
    --max_grad_norm 0.3 \
    --max_new_tokens 32 \
    --source_max_len 512 \
    --target_max_len 512 \
    --lora_r 64 \
    --lora_alpha 16 \
    --lora_dropout 0.1 \
    --double_quant \
    --quant_type nf4 \
    --fp16 \
    --bits 4 \
    --gradient_checkpointing \
    --trust_remote_code \
    --do_train \
    --do_eval \
    --sample_generate \
    --data_seed 42 \
    --seed 0
```

è¦æŸ¥æ‰¾æ›´å¤šç”¨äºå¾®è°ƒå’Œæ¨ç†çš„è„šæœ¬ï¼Œè¯·å‚é˜…è¯¥`scripts`æ–‡ä»¶å¤¹ã€‚

## é‡åŒ–

`BitsandbytesConfig`é‡åŒ–å‚æ•°ç”±ï¼ˆ[å‚è§ huggingface æ–‡æ¡£](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig)ï¼‰æ§åˆ¶ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

- 4 ä½åŠ è½½é€šè¿‡ä»¥ä¸‹æ–¹å¼æ¿€æ´»`load_in_4bit`
- ç”¨äºçº¿æ€§å±‚è®¡ç®—çš„æ•°æ®ç±»å‹`bnb_4bit_compute_dtype`
- åµŒå¥—é‡åŒ–é€šè¿‡ä»¥ä¸‹æ–¹å¼æ¿€æ´»`bnb_4bit_use_double_quant`
- ç”¨äºé‡åŒ–çš„æ•°æ®ç±»å‹ç”± æŒ‡å®š`bnb_4bit_quant_type`ã€‚è¯·æ³¨æ„ï¼Œæœ‰ä¸¤ç§æ”¯æŒçš„é‡åŒ–æ•°æ®ç±»å‹`fp4`ï¼ˆå››ä½æµ®ç‚¹ï¼‰å’Œ`nf4`ï¼ˆæ™®é€šå››ä½æµ®ç‚¹ï¼‰ã€‚åè€…ç†è®ºä¸Šå¯¹äºæ­£æ€åˆ†å¸ƒæƒé‡æ¥è¯´æ˜¯æœ€ä½³çš„ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`nf4`ã€‚

```
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path='/name/or/path/to/your/model',
        load_in_4bit=True,
        device_map='auto',
        max_memory=max_memory,
        torch_dtype=torch.bfloat16,
        quantization_config=BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type='nf4'
        ),
    )
```

## æ•™ç¨‹å’Œæ¼”ç¤º

æˆ‘ä»¬æä¾›äº†ä¸¤ä¸ª Google Colab ç¬”è®°æœ¬æ¥æ¼”ç¤º 4 ä½æ¨¡å‹åœ¨æ¨ç†å’Œå¾®è°ƒä¸­çš„ä½¿ç”¨ã€‚è¿™äº›ç¬”è®°æœ¬æ—¨åœ¨æˆä¸ºè¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘çš„èµ·ç‚¹ã€‚

- [Basic usage Google Colab notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing) è¯¥ç¬”è®°æœ¬å±•ç¤ºäº†å¦‚ä½•åœ¨æ¨ç†ä¸­ä½¿ç”¨ 4 ä½æ¨¡å‹åŠå…¶æ‰€æœ‰å˜ä½“ï¼Œä»¥åŠå¦‚ä½•åœ¨å…è´¹çš„ Google Colab å®ä¾‹ä¸Šè¿è¡Œ GPT-neo-Xï¼ˆ20B å‚æ•°æ¨¡å‹ï¼‰ğŸ¤¯
- [Fine tuning Google Colab notebook](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)  è¯¥ç¬”è®°æœ¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å¾®è°ƒ 4 ä½æ¨¡å‹ã€‚æˆ‘ä»¬è¯æ˜å¯ä»¥åœ¨ Google Colab å®ä¾‹ä¸Šå¾®è°ƒ GPT-neo-X 20Bï¼

å…¶ä»–ç¤ºä¾‹å¯ä»¥åœ¨ç¤ºä¾‹/æ–‡ä»¶å¤¹ä¸‹æ‰¾åˆ°ã€‚

- å¾®è°ƒ LLama-7B (ex1)
- å¾®è°ƒ GPT-neo-X 20B (ex2)

## å¤šGPUè®­ç»ƒ

Hugging Face çš„ Accelerate å¯ä»¥å¼€ç®±å³ç”¨åœ°è¿›è¡Œå¤š GPU è®­ç»ƒå’Œæ¨ç†ã€‚è¯·æ³¨æ„ï¼Œper_device_train_batch_size å’Œ per_device_eval_batch_size å‚æ•°æ˜¯å…¨å±€æ‰¹é‡å¤§å°ï¼Œä¸å…¶åç§°æ‰€æš—ç¤ºçš„ä¸åŒã€‚

å½“åŠ è½½æ¨¡å‹ä»¥åœ¨å¤šä¸ª GPU ä¸Šè¿›è¡Œè®­ç»ƒæˆ–æ¨ç†æ—¶ï¼Œæ‚¨åº”è¯¥å°†ç±»ä¼¼ä»¥ä¸‹å†…å®¹ä¼ é€’ç»™ AutoModelForCausalLM.from_pretrained()ï¼š

```
device_map = "auto"
max_memory = {i: '46000MB' for i in range(torch.cuda.device_count())}
```

## æ¨ç†

### ç»ˆç«¯äº¤äº’å¼å¯¹è¯

è¿è¡Œä¸‹é¢çš„è„šæœ¬ï¼Œç¨‹åºä¼šåœ¨å‘½ä»¤è¡Œä¸­å’Œä½ çš„ChatBotè¿›è¡Œäº¤äº’å¼çš„å¯¹è¯ï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥æŒ‡ç¤ºå¹¶å›è½¦å³å¯ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ `clear` å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²ï¼Œè¾“å…¥ `stop` ç»ˆæ­¢ç¨‹åºã€‚

```
python cli_demo.py \
    --model_name_or_path ~/checkpoints/baichuan7b \ # base model
    --checkpoint_dir ./work_dir/checkpoint-700  \ # è®­ç»ƒçš„æ¨¡å‹æƒé‡
    --trust_remote_code  \
    --double_quant \
    --quant_type nf4 \
    --fp16 \
    --bits 4
```

### ä½¿ç”¨Gradioè¿›è¡Œç½‘é¡µç«¯äº¤äº’

è¯¥æ–‡ä»¶ä» Hugging Face æ¨¡å‹ä¸­å¿ƒè¯»å–åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä» `path/to/your/model_dir` è¯»å– LoRA æƒé‡ï¼Œè¿è¡Œ Gradio æ¥å£ä»¥å¯¹æŒ‡å®šè¾“å…¥è¿›è¡Œæ¨ç†ã€‚ç”¨æˆ·åº”å°†æ­¤è§†ä¸ºæ¨¡å‹ä½¿ç”¨çš„ç¤ºä¾‹ä»£ç ï¼Œå¹¶æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚

ç”¨æ³•ç¤ºä¾‹ï¼š

```
python gradio_webserver.py \
    --model_name_or_path decapoda-research/llama-7b-hf \
    --lora_model_name_or_path  `path/to/your/model_dir`
```

## License

`Efficient Finetuning of Quantized LLMs`æ ¹æ® Apache 2.0 è®¸å¯è¯å‘å¸ƒã€‚

## è‡´è°¢

æˆ‘ä»¬æ„Ÿè°¢ Huggingface å›¢é˜Ÿï¼Œç‰¹åˆ«æ˜¯ Younes Belkadaï¼Œæ„Ÿè°¢ä»–ä»¬æ”¯æŒå°† QLoRA ä¸ PEFT å’Œ Transformer åº“é›†æˆã€‚

æˆ‘ä»¬æ„Ÿè°¢è®¸å¤šå¼€æºè´¡çŒ®è€…çš„å·¥ä½œï¼Œç‰¹åˆ«æ˜¯ï¼š

- [Alpaca-LoRA](https://github.com/tloen/alpaca-lora/)
- [LoRA](https://github.com/microsoft/LoRA/)
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca/)
- [Hugging Face](https://huggingface.co/)
- [LLaMa](https://github.com/facebookresearch/llama/)
- [Vicuna](https://github.com/lm-sys/FastChat/)

## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨æ­¤å­˜å‚¨åº“ä¸­çš„æ•°æ®æˆ–ä»£ç ï¼Œè¯·å¼•ç”¨è¯¥å­˜å‚¨åº“ã€‚

```
@misc{Chinese-Guanaco,
  author = {jianzhnie},
  title = {Chinese-Guanaco: Efficient Finetuning of Quantized LLMs for Chinese},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/jianzhnie/Efficient-Tuning-LLMs}},
}
```
